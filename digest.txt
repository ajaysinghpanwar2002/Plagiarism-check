Directory structure:
└── /src/
    ├── simhash/
    │   └── simhash.go
    ├── config/
    │   └── config.go
    ├── parser/
    │   └── parser.go
    ├── storage/
    │   └── redis_client.go
    ├── sources/
    │   ├── s3.go
    │   └── athena.go
    ├── monitoring/
    │   └── statsd.go
    └── main.go

================================================
File: /simhash/simhash.go
================================================
package simhash

import (
	"crypto/md5"
	"fmt"
	"regexp"
	"strings"
)

type Simhash struct {
	Low  uint64
	High uint64
}

var unicodeWordRegex = regexp.MustCompile(`[\pL\p{Nd}]+`)

func tokenizeUnicode(text string) []string {
	lower := strings.ToLower(text)
	words := unicodeWordRegex.FindAllString(lower, -1)
	return words
}

func New(text string) Simhash {
	tokens := tokenizeUnicode(text)
	if len(tokens) == 0 {
		return Simhash{Low: 0, High: 0}
	}

	weights := make(map[string]int, len(tokens))
	for _, tok := range tokens {
		weights[tok]++
	}

	var vec [128]int

	for tok, w := range weights {
		hash := md5.Sum([]byte(tok))
		for byteIdx, b := range hash {
			for bit := 0; bit < 8; bit++ {
				idx := byteIdx*8 + bit
				if (b>>bit)&1 == 1 {
					vec[idx] += w
				} else {
					vec[idx] -= w
				}
			}
		}
	}

	var low, high uint64
	for i, v := range vec {
		if v > 0 {
			if i < 64 {
				low |= 1 << uint(i)
			} else {
				high |= 1 << uint(i-64)
			}
		}
	}

	return Simhash{Low: low, High: high}
}

func (s Simhash) String() string {
	return fmt.Sprintf("%016x%016x", s.High, s.Low)
}

func HammingDistance(s1, s2 Simhash) int {
	popCount := func(n uint64) int {
		count := 0
		for n > 0 {
			n &= (n - 1) // Brian Kernighan's algorithm to count set bits
			count++
		}
		return count
	}
	return popCount(s1.Low^s2.Low) + popCount(s1.High^s2.High)
}

func ParseSimhashFromString(s string) (Simhash, error) {
	var sh Simhash
	if len(s) != 32 {
		return sh, fmt.Errorf("invalid simhash string length: expected 32, got %d", len(s))
	}
	_, err := fmt.Sscanf(s, "%016x%016x", &sh.High, &sh.Low)
	if err != nil {
		return sh, fmt.Errorf("failed to parse simhash string: %w", err)
	}
	return sh, nil
}


================================================
File: /config/config.go
================================================
package config

import (
	"fmt"
	"os"

	"github.com/joho/godotenv"
)

type Config struct {
	AthenaDatabase       string
	AthenaResultsBucket  string
	AthenaOutputPrefix   string
	AWSRegion            string
	Languages            []string
	NumWorkers           int
	WorkerChannelSize    int
	StoryS3Bucket        string
	RedisAddr            string
	RedisPassword        string
	RedisDB              int
	StatsDHost           string
	StatsDPort           string
	StatsDPrefix         string
	AthenaFetchStartDate string
}

func LoadConfig() (*Config, error) {
	if err := godotenv.Load(); err != nil {
		return nil, fmt.Errorf("loading .env file: %w", err)
	}

	config := &Config{
		AthenaDatabase:       os.Getenv("ATHENA_DATABASE"),
		AthenaResultsBucket:  os.Getenv("ATHENA_RESULTS_BUCKET"),
		AthenaOutputPrefix:   os.Getenv("ATHENA_OUTPUT_PREFIX"),
		AWSRegion:            os.Getenv("AWS_REGION"),
		StoryS3Bucket:        os.Getenv("STORY_S3_BUCKET"),
		Languages:            []string{"HINDI", "ENGLISH", "TAMIL", "TELUGU", "KANNADA", "MALAYALAM", "BENGALI", "MARATHI", "GUJARATI", "ODIA", "PUNJABI"},
		NumWorkers:           10,
		WorkerChannelSize:    1000,
		RedisAddr:            os.Getenv("REDIS_ADDR"),
		RedisPassword:        os.Getenv("REDIS_PASSWORD"),
		RedisDB:              1,
		StatsDHost:           os.Getenv("STATSD_HOST"),
		StatsDPort:           os.Getenv("STATSD_PORT"),
		StatsDPrefix:         os.Getenv("STATSD_PREFIX"),
		AthenaFetchStartDate: os.Getenv("ATHENA_FETCH_START_DATE"),
	}

	if config.AthenaDatabase == "" || config.AthenaResultsBucket == "" ||
		config.AthenaOutputPrefix == "" || config.AWSRegion == "" || config.RedisAddr == "" ||
		config.StatsDHost == "" || config.StatsDPort == "" || config.StatsDPrefix == "" {
		return nil, fmt.Errorf("missing required environment variables for AWS or Redis")
	}

	return config, nil
}


================================================
File: /parser/parser.go
================================================
package parser

import (
	"bytes"
	"encoding/json"
	"golang.org/x/net/html"
	"regexp"
	"strings"
)

type Chapter struct {
	Pages []Page `json:"pages"`
}

type Page struct {
	Pagelets []Pagelet `json:"pagelets"`
}

type Pagelet struct {
	Type string `json:"type"`
	Data string `json:"data"`
}

var (
	brRegex              = regexp.MustCompile(`(?i)<(br|p)\s*/?>`)
	multipleSpacesRegex  = regexp.MustCompile(`\s+`)
	specialCharsRegex    = regexp.MustCompile(`&#\d+;`)
	nonBreakingSpaceRegex = regexp.MustCompile(`&nbsp;`)
)

func cleanHTML(htmlBody string) string {
	doc, err := html.Parse(strings.NewReader(htmlBody))
	if err != nil {
		return htmlBody
	}

	var buf bytes.Buffer
	var extractText func(*html.Node)
	extractText = func(n *html.Node) {
		if n.Type == html.TextNode {
			buf.WriteString(n.Data)
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			extractText(c)
		}
	}

	extractText(doc)
	return buf.String()
}

func preProcessText(text string) string {
	processedText := brRegex.ReplaceAllString(text, "\n")
	processedText = cleanHTML(processedText)
	processedText = html.UnescapeString(processedText)
	processedText = specialCharsRegex.ReplaceAllString(processedText, "")
	processedText = nonBreakingSpaceRegex.ReplaceAllString(processedText, " ")
	processedText = multipleSpacesRegex.ReplaceAllString(processedText, " ")

	return strings.TrimSpace(processedText)
}

func Parse(chapterContent []byte) (string, error) {
	var chapter Chapter
	if err := json.Unmarshal(chapterContent, &chapter); err != nil {
		return preProcessText(string(chapterContent)), nil
	}

	var contentBuilder strings.Builder
	for _, page := range chapter.Pages {
		for _, pagelet := range page.Pagelets {
			if strings.ToUpper(pagelet.Type) == "HTML" || strings.ToUpper(pagelet.Type) == "TEXT" {
				if pagelet.Data != "" {
					cleanedText := preProcessText(pagelet.Data)
					if cleanedText != "" {
						contentBuilder.WriteString(cleanedText)
						contentBuilder.WriteString(" ")
					}
				}
			}
		}
	}

	return strings.TrimSpace(contentBuilder.String()), nil
}


================================================
File: /storage/redis_client.go
================================================
package storage

import (
	"context"
	"fmt"
	"log"
	"strconv"
	"strings"
	"sync"
	"time"

	"plagiarism-detector/src/simhash"

	"github.com/cactus/go-statsd-client/v5/statsd"
	"github.com/redis/go-redis/v9"
	"plagiarism-detector/src/monitoring"
)

const (
	numBands                   = 8
	bandBitSize                = 16
	bandMask                   = uint64(1<<bandBitSize) - 1
	hammingDistanceThreshold   = 3
	checkpointKeyFormat        = "plagiarism_detector:checkpoint:athena_fetch_all:%s:offset"
	checkpointTTL              = 7 * 24 * time.Hour
	defaultSimhashBatchSize    = 500
	defaultSimhashBatchTimeout = 5 * time.Second
	simhashChannelCapacity     = 1000
)

type SimhashData struct {
	PratilipiID string
	Language    string
	Hash        simhash.Simhash
}

type RedisClient struct {
	client           *redis.Client
	statsdClient     statsd.Statter
	simhashBatchChan chan SimhashData
	storerWg         sync.WaitGroup
	storerCancelFunc context.CancelFunc
	batchSize        int
	batchTimeout     time.Duration
}

func NewRedisClient(parentCtx context.Context, addr, password string, db int, statsdClient statsd.Statter) (*RedisClient, error) {
	rdb := redis.NewClient(&redis.Options{
		Addr:     addr,
		Password: password,
		DB:       db,
	})

	if _, err := rdb.Ping(parentCtx).Result(); err != nil {
		return nil, fmt.Errorf("failed to connect to Redis: %w", err)
	}

	storerCtx, storerCancel := context.WithCancel(parentCtx)

	rc := &RedisClient{
		client:           rdb,
		statsdClient:     statsdClient,
		simhashBatchChan: make(chan SimhashData, simhashChannelCapacity),
		storerCancelFunc: storerCancel,
		batchSize:        defaultSimhashBatchSize,
		batchTimeout:     defaultSimhashBatchTimeout,
	}

	rc.storerWg.Add(1)
	go rc.runSimhashStorer(storerCtx)

	return rc, nil
}

func (rc *RedisClient) runSimhashStorer(ctx context.Context) {
	defer rc.storerWg.Done()
	batch := make([]SimhashData, 0, rc.batchSize)

	for {
		select {
		case <-ctx.Done():
			log.Println("Simhash storer: context cancelled, flushing remaining items and exiting.")
			if len(batch) > 0 {
				flushCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
				rc.flushSimhashBatch(flushCtx, batch)
				cancel()
			}
			return
		case data, ok := <-rc.simhashBatchChan:
			if !ok {
				log.Println("Simhash storer: channel closed, flushing remaining items and exiting.")
				if len(batch) > 0 {
					flushCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
					rc.flushSimhashBatch(flushCtx, batch)
					cancel()
				}
				return
			}
			batch = append(batch, data)
			if len(batch) >= rc.batchSize {
				log.Printf("Simhash storer: batch size reached (%d), flushing.", len(batch))
				flushCtx, cancel := context.WithTimeout(context.Background(), 15*time.Second)
				rc.flushSimhashBatch(flushCtx, batch)
				cancel()
				batch = make([]SimhashData, 0, rc.batchSize)
			}
		}
	}
}

func (rc *RedisClient) flushSimhashBatch(ctx context.Context, batch []SimhashData) error {
	if len(batch) == 0 {
		return nil
	}
	pipe := rc.client.Pipeline()
	processedIDs := make([]string, 0, len(batch))

	for _, data := range batch {
		fullHashKey := fmt.Sprintf("simhashes:%s", strings.ToUpper(data.Language))
		pipe.HSet(ctx, fullHashKey, data.PratilipiID, data.Hash.String())

		for i := 0; i < numBands; i++ {
			var bandValue uint64
			if i < numBands/2 { // First 4 bands from Low
				shift := uint(i * bandBitSize)
				bandValue = (data.Hash.Low >> shift) & bandMask
			} else { // Next 4 bands from High
				shift := uint((i - numBands/2) * bandBitSize)
				bandValue = (data.Hash.High >> shift) & bandMask
			}
			bucketKey := fmt.Sprintf("%s:%d:%x", strings.ToUpper(data.Language), i, bandValue)
			pipe.SAdd(ctx, bucketKey, data.PratilipiID)
		}
		processedIDs = append(processedIDs, data.PratilipiID)
	}

	_, err := pipe.Exec(ctx)
	if err != nil {
		log.Printf("ERROR: Failed to execute Redis pipeline for storing simhash batch (count: %d): %v. IDs: %v", len(batch), err, processedIDs)
		monitoring.Increment("failed-store-simhash-batch", rc.statsdClient)
		return fmt.Errorf("failed to execute Redis pipeline for storing simhash batch: %w", err)
	}

	log.Printf("Successfully stored batch of %d SimHashes in Redis. First ID: %s", len(batch), batch[0].PratilipiID)
	monitoring.Increment("stored-simhash-batch", rc.statsdClient)
	return nil
}

// It performs the actual storage of the simhash and its bands
func (rc *RedisClient) storeSimhashInternal(ctx context.Context, pratilipiID, language string, hash simhash.Simhash) error {
	pipe := rc.client.Pipeline()

	fullHashKey := fmt.Sprintf("simhashes:%s", strings.ToUpper(language))
	pipe.HSet(ctx, fullHashKey, pratilipiID, hash.String())

	for i := 0; i < numBands; i++ {
		var bandValue uint64
		if i < numBands/2 { // First 4 bands from Low
			shift := uint(i * bandBitSize)
			bandValue = (hash.Low >> shift) & bandMask
		} else { // Next 4 bands from High
			shift := uint((i - numBands/2) * bandBitSize)
			bandValue = (hash.High >> shift) & bandMask
		}

		bucketKey := fmt.Sprintf("%s:%d:%x", strings.ToUpper(language), i, bandValue)
		pipe.SAdd(ctx, bucketKey, pratilipiID)
	}

	_, err := pipe.Exec(ctx)
	if err != nil {
		monitoring.Increment("failed-store-simhash", rc.statsdClient)
		return fmt.Errorf("failed to execute Redis pipeline for storing simhash for ID %s: %w", pratilipiID, err)
	}
	log.Printf("Successfully stored SimHash for Pratilipi ID %s (lang: %s) in Redis", pratilipiID, language)
	monitoring.Increment("stored-simhash", rc.statsdClient)
	return nil
}

func (rc *RedisClient) CheckAndStoreSimhash(ctx context.Context, pratilipiID, language string, newHash simhash.Simhash) (bool, error) {
	bucketKeys := make([]string, numBands)
	if numBands > 0 {
		for i := 0; i < numBands; i++ {
			var bandValue uint64
			if i < numBands/2 {
				shift := uint(i * bandBitSize)
				bandValue = (newHash.Low >> shift) & bandMask
			} else {
				shift := uint((i - numBands/2) * bandBitSize)
				bandValue = (newHash.High >> shift) & bandMask
			}
			bucketKeys[i] = fmt.Sprintf("%s:%d:%x", strings.ToUpper(language), i, bandValue)
		}
	}

	var candidateIDs []string
	var err error
	if len(bucketKeys) > 0 {
		candidateIDs, err = rc.client.SUnion(ctx, bucketKeys...).Result()
		if err != nil {
			return false, fmt.Errorf("failed to get candidate IDs using SUnion for %s: %w", pratilipiID, err)
		}
	}

	fullHashKey := fmt.Sprintf("simhashes:%s", strings.ToUpper(language))

	for _, candidateID := range candidateIDs {
		if candidateID == pratilipiID {
			continue
		}

		candidateHashStr, err := rc.client.HGet(ctx, fullHashKey, candidateID).Result()
		if err == redis.Nil {
			log.Printf("WARN: Candidate ID %s found in LSH bucket but not in full hash map %s. Skipping.", candidateID, fullHashKey)
			monitoring.Increment("candidate-id-not-found-in-full-hash", rc.statsdClient)
			continue
		}
		if err != nil {
			log.Printf("WARN: Failed to get full hash for candidate ID %s from %s: %v. Skipping.", candidateID, fullHashKey, err)
			monitoring.Increment("failed-get-full-hash", rc.statsdClient)
			continue
		}

		candidateHash, err := simhash.ParseSimhashFromString(candidateHashStr)
		if err != nil {
			log.Printf("WARN: Failed to parse Simhash string '%s' for candidate ID %s: %v. Skipping.", candidateHashStr, candidateID, err)
			continue
		}

		distance := simhash.HammingDistance(newHash, candidateHash)

		if distance <= hammingDistanceThreshold {
			log.Printf("Potential plagiarism DETECTED for Pratilipi ID %s (lang: %s). Similar to %s. Hamming Distance: %d",
				pratilipiID, language, candidateID, distance)
			_, err = rc.client.SAdd(ctx, fmt.Sprintf("potential_plagiarism:%s", strings.ToUpper(language)), pratilipiID, candidateID).Result()
			return true, nil // Plagiarism detected
		}
	}

	dataToStore := SimhashData{PratilipiID: pratilipiID, Language: language, Hash: newHash}
	select {
	case rc.simhashBatchChan <- dataToStore:
	case <-ctx.Done():
		log.Printf("ERROR: Context cancelled before queuing SimHash for Pratilipi ID %s (lang: %s): %v", pratilipiID, language, ctx.Err())
		return false, fmt.Errorf("context cancelled before queuing simhash for ID %s: %w", pratilipiID, ctx.Err())
	}
	return false, nil
}

func (rc *RedisClient) Close() error {
	log.Println("Closing RedisClient: signalling simhash storer to stop...")
	if rc.storerCancelFunc != nil {
		rc.storerCancelFunc()
	}

	rc.storerWg.Wait()
	log.Println("Simhash storer finished.")

	log.Println("Closing Redis connection.")
	return rc.client.Close()
}

func (rc *RedisClient) GetCheckpointOffset(ctx context.Context, language string) (int64, error) {
	key := fmt.Sprintf(checkpointKeyFormat, strings.ToUpper(language))
	val, err := rc.client.Get(ctx, key).Result()
	if err == redis.Nil {
		return 0, nil
	}
	if err != nil {
		return 0, fmt.Errorf("failed to get checkpoint offset for language %s from Redis: %w", language, err)
	}
	offset, err := strconv.ParseInt(val, 10, 64)
	if err != nil {
		log.Printf("WARN: Failed to parse checkpoint offset '%s' for language %s: %v. Defaulting to 0.", val, language, err)
		return 0, nil
	}
	return offset, nil
}

func (rc *RedisClient) SetCheckpointOffset(ctx context.Context, language string, offset int64) error {
	key := fmt.Sprintf(checkpointKeyFormat, strings.ToUpper(language))
	err := rc.client.Set(ctx, key, offset, checkpointTTL).Err()
	if err != nil {
		return fmt.Errorf("failed to set checkpoint offset for language %s to %d in Redis: %w", language, offset, err)
	}
	return nil
}

func (rc *RedisClient) DeleteCheckpointOffset(ctx context.Context, language string) error {
	key := fmt.Sprintf(checkpointKeyFormat, strings.ToUpper(language))
	err := rc.client.Del(ctx, key).Err()
	if err != nil && err != redis.Nil {
		return fmt.Errorf("failed to delete checkpoint offset for language %s from Redis: %w", language, err)
	}
	log.Printf("Deleted checkpoint for language %s (if existed)", strings.ToUpper(language))
	return nil
}


================================================
File: /sources/s3.go
================================================
package sources

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"path/filepath"

	"plagiarism-detector/src/monitoring"
	"plagiarism-detector/src/parser"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	"github.com/cactus/go-statsd-client/v5/statsd"
)

type IndexFile struct {
	Chapters         []string `json:"chapters"`
	DeletedChapters  []string `json:"deletedChapters"`
	NotFoundChapters []string `json:"notFoundChapters"`
	DocVersion       string   `json:"docVersion"`
	ID               int64    `json:"_id"`
	LastUpdated      string   `json:"lastUpdated"`
	CreatedAt        string   `json:"createdAt"`
	V                int      `json:"__v"`
	LastBackupAt     string   `json:"lastBackupAt"`
}

type S3Downloader struct {
	s3Client      *s3.Client
	storyS3Bucket string
	statsdClient  statsd.Statter
}

func NewS3Downloader(ctx context.Context, region, storyS3Bucket string, statsdClient statsd.Statter) (*S3Downloader, error) {
	cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(region))
	if err != nil {
		return nil, fmt.Errorf("failed to load AWS config: %w", err)
	}

	return &S3Downloader{
		s3Client:      s3.NewFromConfig(cfg),
		storyS3Bucket: storyS3Bucket,
		statsdClient:  statsdClient,
	}, nil
}

func (s *S3Downloader) DownloadStoryContent(ctx context.Context, pratilipiID string) (string, error) {
	indexPath := filepath.Join(pratilipiID, "index")

	indexObj, err := s.s3Client.GetObject(ctx, &s3.GetObjectInput{
		Bucket: aws.String(s.storyS3Bucket),
		Key:    aws.String(indexPath),
	})
	if err != nil {
		monitoring.Increment("failed-download-index", s.statsdClient)
		return "", fmt.Errorf("failed to download index file %s for pratilipi %s: %w", indexPath, pratilipiID, err)
	}
	defer indexObj.Body.Close()

	indexBody, err := io.ReadAll(indexObj.Body)
	if err != nil {
		return "", fmt.Errorf("failed to read index file body for pratilipi %s: %w", pratilipiID, err)
	}

	var indexData IndexFile
	if err := json.Unmarshal(indexBody, &indexData); err != nil {
		return "", fmt.Errorf("failed to unmarshal index data for pratilipi %s: %w", pratilipiID, err)
	}

	if len(indexData.Chapters) == 0 {
		log.Printf("No chapters found for pratilipi ID %s", pratilipiID)
		monitoring.Increment("no-chapters-found", s.statsdClient)
		return "", nil
	}

	var contentBuffer bytes.Buffer
	for _, chapterID := range indexData.Chapters {
		chapterPath := filepath.Join(pratilipiID, "chapters", chapterID)
		chapterObj, err := s.s3Client.GetObject(ctx, &s3.GetObjectInput{
			Bucket: aws.String(s.storyS3Bucket),
			Key:    aws.String(chapterPath),
		})
		if err != nil {
			log.Printf("WARN: Failed to download chapter %s for pratilipi %s: %v. Skipping chapter.", chapterPath, pratilipiID, err)
			monitoring.Increment("failed-download-chapter", s.statsdClient)
			continue
		}
		defer chapterObj.Body.Close()

		chapterBody, err := io.ReadAll(chapterObj.Body)
		if err != nil {
			log.Printf("WARN: Failed to read chapter body %s for pratilipi %s: %v. Skipping chapter.", chapterPath, pratilipiID, err)
			continue
		}

		cleanedText, err := parser.Parse(chapterBody)
		if err != nil {
			log.Printf("WARN: Failed to parse chapter %s for pratilipi %s: %v. Skipping chapter.", chapterPath, pratilipiID, err)
			continue
		}

		if cleanedText != "" {
			contentBuffer.WriteString(cleanedText)
			contentBuffer.WriteString("\n")
		}
	}

	return contentBuffer.String(), nil
}


================================================
File: /sources/athena.go
================================================
package sources

import (
	"context"
	"encoding/csv"
	"fmt"
	"io"
	"log"
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/service/athena"
	"github.com/aws/aws-sdk-go-v2/service/athena/types"
	"github.com/aws/aws-sdk-go-v2/service/s3"
	"github.com/cactus/go-statsd-client/v5/statsd"
	"plagiarism-detector/src/monitoring"
	"plagiarism-detector/src/storage"
)

type PratilipiData struct {
	ID       string
	Language string
}

type AthenaProcessor struct {
	athenaClient   *athena.Client
	s3Client       *s3.Client
	s3Bucket       string
	database       string
	outputPrefix   string
	statsdClient   statsd.Statter
	redisClient    *storage.RedisClient
	fetchStartDate string
}

func NewAthenaProcessor(ctx context.Context, region, s3Bucket, outputPrefix, database, fetchStartDate string, statsdClient statsd.Statter, redisClient *storage.RedisClient) (*AthenaProcessor, error) {
	cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(region))
	if err != nil {
		return nil, fmt.Errorf("failed to load AWS config: %w", err)
	}

	return &AthenaProcessor{
		athenaClient:   athena.NewFromConfig(cfg),
		s3Client:       s3.NewFromConfig(cfg),
		s3Bucket:       s3Bucket,
		outputPrefix:   outputPrefix,
		database:       database,
		statsdClient:   statsdClient,
		redisClient:    redisClient,
		fetchStartDate: fetchStartDate,
	}, nil
}

func (a *AthenaProcessor) executeQueryAndProcess(ctx context.Context, query string) ([]string, error) {
	outputLocation := fmt.Sprintf("s3://%s/%s", a.s3Bucket, a.outputPrefix)
	startInput := &athena.StartQueryExecutionInput{
		QueryString: aws.String(query),
		QueryExecutionContext: &types.QueryExecutionContext{
			Database: aws.String(a.database),
		},
		ResultConfiguration: &types.ResultConfiguration{
			OutputLocation: aws.String(outputLocation),
		},
	}

	startOutput, err := a.athenaClient.StartQueryExecution(ctx, startInput)
	if err != nil {
		return nil, fmt.Errorf("failed to start query execution: %w", err)
	}

	queryExecutionID := *startOutput.QueryExecutionId
	log.Printf("Query execution started with ID: %s", queryExecutionID)

	if err := a.waitForQueryCompletion(ctx, queryExecutionID); err != nil {
		return nil, err
	}

	return a.processQueryResults(ctx, queryExecutionID)
}

func (a *AthenaProcessor) waitForQueryCompletion(ctx context.Context, queryExecutionID string) error {
	maxRetries := 3
	for i := 0; i < maxRetries; i++ {
		input := &athena.GetQueryExecutionInput{
			QueryExecutionId: aws.String(queryExecutionID),
		}

		output, err := a.athenaClient.GetQueryExecution(ctx, input)
		if err != nil {
			return fmt.Errorf("failed to get query execution status: %w", err)
		}

		state := output.QueryExecution.Status.State
		log.Printf("Query execution status: %s", state)

		switch state {
		case types.QueryExecutionStateSucceeded:
			return nil
		case types.QueryExecutionStateFailed, types.QueryExecutionStateCancelled:
			reason := ""
			if output.QueryExecution.Status.StateChangeReason != nil {
				reason = *output.QueryExecution.Status.StateChangeReason
			}
			monitoring.Increment("athena-query-execution-failed", a.statsdClient)
			return fmt.Errorf("query execution failed or cancelled: %s", reason)
		}

		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-time.After(10 * time.Second):
		}
	}

	return fmt.Errorf("query execution timed out after %d retries", maxRetries)
}

func (a *AthenaProcessor) processQueryResults(ctx context.Context, queryExecutionID string) ([]string, error) {
	getExecInput := &athena.GetQueryExecutionInput{
		QueryExecutionId: aws.String(queryExecutionID),
	}

	execOutput, err := a.athenaClient.GetQueryExecution(ctx, getExecInput)
	if err != nil {
		return nil, fmt.Errorf("failed to get query execution details: %w", err)
	}

	s3OutputLocation := *execOutput.QueryExecution.ResultConfiguration.OutputLocation
	s3Key := s3OutputLocation[len(fmt.Sprintf("s3://%s/", a.s3Bucket)):]

	getObjectInput := &s3.GetObjectInput{
		Bucket: aws.String(a.s3Bucket),
		Key:    aws.String(s3Key),
	}

	s3Output, err := a.s3Client.GetObject(ctx, getObjectInput)
	if err != nil {
		return nil, fmt.Errorf("failed to get results from S3: %w", err)
	}
	defer s3Output.Body.Close()

	csvReader := csv.NewReader(s3Output.Body)
	if _, err := csvReader.Read(); err != nil {
		if err == io.EOF {
			return []string{}, nil // Empty result set
		}
		return nil, fmt.Errorf("failed to read CSV header: %w", err)
	}

	var pratilipiIDs []string
	for {
		record, err := csvReader.Read()
		if err == io.EOF {
			break
		}
		if err != nil {
			log.Printf("Error reading CSV record: %v", err)
			continue
		}
		if len(record) > 0 {
			pratilipiIDs = append(pratilipiIDs, record[0])
		}
	}

	log.Printf("Successfully processed %d records from query %s", len(pratilipiIDs), queryExecutionID)
	monitoring.Increment("athena-query-execution-success", a.statsdClient)
	return pratilipiIDs, nil
}

func (a *AthenaProcessor) FetchPublishedPratilipiIDsForTargetDate(ctx context.Context, language string) ([]string, error) {
	now := time.Now()
	loc := now.Location()

	targetDate := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, loc)

	if a.fetchStartDate != "" {
		parsedDate, err := time.Parse("2006-01-02", a.fetchStartDate)
		if err == nil {
			targetDate = time.Date(parsedDate.Year(), parsedDate.Month(), parsedDate.Day(), 0, 0, 0, 0, loc)
		} else {
			log.Printf("WARN: Invalid ATHENA_FETCH_START_DATE format: '%s'. Expected YYYY-MM-DD. Defaulting to today.", a.fetchStartDate)
		}
	}

	queryPeriodStart := targetDate
	queryPeriodEnd := targetDate.AddDate(0, 0, 1) // Day after targetDate, for exclusive upper bound

	queryPeriodStartStr := queryPeriodStart.Format("2006-01-02 15:04:05")
	queryPeriodEndStr := queryPeriodEnd.Format("2006-01-02 15:04:05")

	queryFormat := "SELECT id FROM pratilipi_pratilipi WHERE language='%s' AND content_type='PRATILIPI' AND state='PUBLISHED' AND published_at >= to_unixtime(parse_datetime('%s','yyyy-MM-dd HH:mm:ss')) AND published_at < to_unixtime(parse_datetime('%s','yyyy-MM-dd HH:mm:ss'))"
	query := fmt.Sprintf(queryFormat, language, queryPeriodStartStr, queryPeriodEndStr)

	log.Printf("Executing query for target date %s: %s", targetDate.Format("2006-01-02"), query)
	return a.executeQueryAndProcess(ctx, query)
}

func (a *AthenaProcessor) FetchPublishedPratilipiIDs(
	ctx context.Context,
	language string,
	taskChannel chan<- PratilipiData,
) error {
	initialOffset, err := a.redisClient.GetCheckpointOffset(ctx, language)
	if err != nil {
		log.Printf("WARN: Failed to get checkpoint for %s, starting from offset 0: %v", language, err)
		initialOffset = 0
	}
	log.Printf("Resuming FetchPublishedPratilipiIDs for %s from offset %d", language, initialOffset)

	dateFilterClause := ""
	if a.fetchStartDate != "" {
		_, err := time.Parse("2006-01-02", a.fetchStartDate)
		if err != nil {
			log.Printf("WARN: Invalid ATHENA_FETCH_START_DATE format '%s'. Expected 'YYYY-MM-DD'. The date filter will NOT be applied.", a.fetchStartDate)
		} else {
			startDateForQuery := fmt.Sprintf("%s 00:00:00", a.fetchStartDate)
			dateFilterClause = fmt.Sprintf(" AND published_at >= to_unixtime(parse_datetime('%s','yyyy-MM-dd HH:mm:ss'))", startDateForQuery)
			log.Printf("Applying start date filter: fetching content published on or after %s for language %s.", a.fetchStartDate, language)
			log.Printf("NOTE: Using a start date filter with offset-based checkpointing assumes the set of historical data is stable. If you change the start date, consider deleting the existing Redis checkpoints for this language to force a full re-fetch from the new date.")
		}
	}

	currentQueryOffset := initialOffset
	var itemsSentThisRun int64 = 0
	var itemsSentSinceLastSuccessfulCheckpoint int64 = 0

	const batchSize = 50000
	const checkpointUpdateCountThreshold = 10000

	defer func() {
		if itemsSentSinceLastSuccessfulCheckpoint > 0 {
			finalCheckpointValue := initialOffset + itemsSentThisRun
			log.Printf("Attempting to set final checkpoint for %s at offset %d on exit. (Items sent this run: %d, items since last checkpoint: %d)",
				language, finalCheckpointValue, itemsSentThisRun, itemsSentSinceLastSuccessfulCheckpoint)

			saveCtx, cancelSave := context.WithTimeout(context.Background(), 30*time.Second)
			defer cancelSave()

			if err := a.redisClient.SetCheckpointOffset(saveCtx, language, finalCheckpointValue); err != nil {
				log.Printf("ERROR: Failed to set final checkpoint for %s at offset %d: %v", language, finalCheckpointValue, err)
			} else {
				log.Printf("Successfully set final checkpoint for %s at offset %d on exit", language, finalCheckpointValue)
			}
		} else if itemsSentThisRun > 0 {
			log.Printf("No new items processed since last successful checkpoint for %s. Final checkpoint not updated. (Total items sent this run: %d)", language, itemsSentThisRun)
		}
	}()

	for {
		select {
		case <-ctx.Done():
			log.Printf("Context cancelled, stopping FetchPublishedPratilipiIDs for %s.", language)
			return ctx.Err()
		default:
			// Continue if context is not cancelled.
		}

		query := fmt.Sprintf(
			"SELECT id FROM pratilipi_pratilipi WHERE language='%s' AND content_type='PRATILIPI' AND state='PUBLISHED'%s ORDER BY id DESC LIMIT %d OFFSET %d",
			language,
			dateFilterClause,
			batchSize,
			currentQueryOffset,
		)

		log.Printf("Fetching batch for %s with query: %s", language, query)
		batchIDs, err := a.executeQueryAndProcess(ctx, query)
		if err != nil {
			log.Printf("ERROR: Failed to execute query for batch (lang: %s, offset: %d): %v. Retrying after delay.", language, currentQueryOffset, err)
			select {
			case <-time.After(10 * time.Second):
				continue
			case <-ctx.Done():
				log.Printf("Context cancelled while waiting to retry query for %s.", language)
				return ctx.Err()
			}
		}

		if len(batchIDs) == 0 {
			log.Printf("No more IDs found for %s at offset %d. Fetch completed.", language, currentQueryOffset)
			break
		}

		log.Printf("Fetched %d IDs for %s (offset: %d)", len(batchIDs), language, currentQueryOffset)

		for _, id := range batchIDs {
			task := PratilipiData{ID: id, Language: language}
			select {
			case taskChannel <- task:
				itemsSentThisRun++
				itemsSentSinceLastSuccessfulCheckpoint++
			case <-ctx.Done():
				log.Printf("Context cancelled while sending task for ID %s, language %s.", id, language)
				return ctx.Err()
			}
		}

		currentQueryOffset += int64(len(batchIDs))

		checkpointValueToStore := initialOffset + itemsSentThisRun
		if itemsSentSinceLastSuccessfulCheckpoint >= checkpointUpdateCountThreshold {
			log.Printf("Attempting to set checkpoint for %s at offset %d. (Items since last checkpoint: %d)",
				language, checkpointValueToStore, itemsSentSinceLastSuccessfulCheckpoint)

			checkpointCtx, checkpointCancel := context.WithTimeout(context.Background(), 15*time.Second)
			err := a.redisClient.SetCheckpointOffset(checkpointCtx, language, checkpointValueToStore)
			checkpointCancel()

			if err != nil {
				log.Printf("ERROR: Failed to set checkpoint for %s at offset %d: %v", language, checkpointValueToStore, err)
			} else {
				log.Printf("Successfully set checkpoint for %s at offset %d", language, checkpointValueToStore)
				itemsSentSinceLastSuccessfulCheckpoint = 0 // Reset counter on successful save.
			}
		}
	}

	log.Printf("Finished fetching all IDs for language %s. Total items sent in this run: %d. Effective next offset for resumption: %d.", language, itemsSentThisRun, initialOffset+itemsSentThisRun)
	return nil
}


================================================
File: /monitoring/statsd.go
================================================
package monitoring

import (
	"fmt"
	"time"

	"github.com/cactus/go-statsd-client/v5/statsd"
)

func ConnectStatsd(host, port, prefix string) (statsd.Statter, error) {
	address := fmt.Sprintf("%s:%s", host, port)

	config := statsd.ClientConfig{
		Address:       address,
		Prefix:        prefix,
		TagFormat:     statsd.InfixComma,
		UseBuffered:   true,
		FlushInterval: 1 * time.Second,
	}

	return statsd.NewClientWithConfig(&config)
}

func Increment(name string, client statsd.Statter) {
	err := client.Inc(name, 1, 0.5)
	if err != nil {
		fmt.Println(err)
	}
}

func Timing(name string, client statsd.Statter, timing time.Time, tags ...statsd.Tag) {
	delta := time.Since(timing).Milliseconds()
	err := client.Timing(name, delta, 1.0, tags...)
	if err != nil {
		fmt.Println(err)
	}
}


================================================
File: /main.go
================================================
package main

import (
	"context"
	"fmt"
	"log"
	"sync"

	"plagiarism-detector/src/config"
	"plagiarism-detector/src/monitoring"
	"plagiarism-detector/src/simhash"
	"plagiarism-detector/src/sources"
	"plagiarism-detector/src/storage"
)

func main() {
	config, err := config.LoadConfig()
	if err != nil {
		log.Fatalf("Failed to load configuration: %v", err)
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	StatsDClient, err := monitoring.ConnectStatsd(config.StatsDHost, config.StatsDPort, config.StatsDPrefix)
	if err != nil {
		log.Println("Unable to connect to grafana", err)
	}
	defer StatsDClient.Close()

	s3Downloader, err := sources.NewS3Downloader(ctx, config.AWSRegion, config.StoryS3Bucket, StatsDClient)
	if err != nil {
		log.Fatalf("Failed to create S3 downloader: %v", err)
	}

	redisClient, err := storage.NewRedisClient(ctx, config.RedisAddr, config.RedisPassword, config.RedisDB, StatsDClient)
	if err != nil {
		log.Fatalf("Failed to create Redis client: %v", err)
	}
	defer redisClient.Close()

	pratilipiTaskChannel := make(chan sources.PratilipiData, config.WorkerChannelSize)

	processor, err := sources.NewAthenaProcessor(
		ctx,
		config.AWSRegion,
		config.AthenaResultsBucket,
		config.AthenaOutputPrefix,
		config.AthenaDatabase,
		config.AthenaFetchStartDate,
		StatsDClient,
		redisClient,
	)
	if err != nil {
		log.Fatalf("Failed to create Athena processor: %v", err)
	}

	var wg sync.WaitGroup

	for i := 0; i < config.NumWorkers; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			for task := range pratilipiTaskChannel {
				log.Printf("Worker %d: Processing Pratilipi ID %s for language %s", workerID, task.ID, task.Language)
				content, err := s3Downloader.DownloadStoryContent(ctx, task.ID)
				if err != nil {
					log.Printf("Worker %d: ERROR downloading and parsing content for Pratilipi ID %s: %v", workerID, task.ID, err)
					continue
				}

				if content == "" {
					log.Printf("Worker %d: No content found for Pratilipi ID %s (or all chapters failed to download/parse)", workerID, task.ID)
					monitoring.Increment("no-content-found-text-empty", StatsDClient)
					continue
				}

				hash := simhash.New(content)

				plagiarismDetected, err := redisClient.CheckAndStoreSimhash(ctx, task.ID, task.Language, hash)
				if err != nil {
					log.Printf("Worker %d: ERROR processing SimHash for Pratilipi ID %s (lang: %s): %v", workerID, task.ID, task.Language, err)
					monitoring.Increment("failed-process-simhash", StatsDClient)
				} else if plagiarismDetected {
					log.Printf("Worker %d: Potential PLAGIARISM DETECTED for Pratilipi ID %s (lang: %s). Not stored.", workerID, task.ID, task.Language)
					monitoring.Increment("potential-plagiarism-detected", StatsDClient)
				}
			}
		}(i)
	}

	go func() {
		defer close(pratilipiTaskChannel)
		for _, language := range config.Languages {
			log.Printf("Producer: Starting to fetch Pratilipi IDs for %s (with checkpointing)", language)
			err := processor.FetchPublishedPratilipiIDs(ctx, language, pratilipiTaskChannel)
			if err != nil {
				if err == context.Canceled || err == context.DeadlineExceeded {
					log.Printf("Producer: Fetching for %s was cancelled or timed out: %v. Stopping producer.", language, err)
					monitoring.Increment("fetch-cancelled-or-timed-out", StatsDClient)
					return
				}
				log.Printf("ERROR: Could not complete fetching IDs for %s: %v. Continuing with next language.", language, err)
				select {
				case <-ctx.Done():
					log.Printf("Producer: Context is done, stopping further language processing.")
					return
				default:
				}
			} else {
				log.Printf("Producer: Finished fetching IDs for %s.", language)
			}
		}
		log.Printf("Producer: All languages processed for ID fetching.")
	}()

	wg.Wait()
	fmt.Println("All Pratilipi IDs have been processed. Application finished.")
}


